{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime, date\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "import lightgbm as lgb\n",
    "from lightgbm import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sunrise, sunset, dawn, noon, dusk\n",
    "from lightgbm import Dataset, train as lgb_train\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义文件夹路径\n",
    "base_dir = './dataset.part1'\n",
    "\n",
    "# 定义场站文件夹名称\n",
    "stations = [f'JSFD{i:03d}' for i in range(1, 15)] + [f'JSGF{i:03d}' for i in range(1, 14)]\n",
    "\n",
    "# 存储每个场站的数据\n",
    "station_data = {}\n",
    "\n",
    "# 遍历所有场站文件夹\n",
    "for station in stations:\n",
    "    station_dir = os.path.join(base_dir, station)\n",
    "    \n",
    "    # 初始化数据框\n",
    "    output_df, wind_df, operation_df = None, None, None\n",
    "    \n",
    "    # 遍历该文件夹下所有的xlsx文件\n",
    "    for file_name in os.listdir(station_dir):\n",
    "        # 跳过以 ~ 开头的临时文件\n",
    "        if file_name.startswith('~$'):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(station_dir, file_name)\n",
    "        \n",
    "        # 读取场站出力.xlsx\n",
    "        if '场站出力' in file_name and file_name.endswith('.xlsx'):\n",
    "            all_sheets = pd.read_excel(file_path, sheet_name=None)  # 读取所有工作表\n",
    "            # 遍历所有工作表\n",
    "            for sheet_name, df in all_sheets.items():\n",
    "                if df.shape[0] > 5 and df.shape[1] >= 2:  # 检查是否有足够的行和列\n",
    "                    df = df.iloc[5:, [0, 1]]  # 从第5行开始取第一列和第二列\n",
    "                    df.columns = ['时间', '出力功率']  # 重命名列\n",
    "                    # 清理时间列，去掉任何可能的无效字符\n",
    "                    df['时间'] = df['时间'].astype(str).str.strip()  # 去除空格\n",
    "                    df['时间'] = pd.to_datetime(df['时间'], errors='coerce')  # 将时间列转换为datetime类型，错误处理为NaT\n",
    "                    if output_df is None:\n",
    "                        output_df = df\n",
    "                    else:\n",
    "                        output_df = pd.concat([output_df, df])\n",
    "                else:\n",
    "                    print(f\"文件 {file_path} 的工作表 {sheet_name} 数据不足，跳过处理。\")\n",
    "            print(f\"读取文件: {file_path}\")\n",
    "\n",
    "        # 读取测风数据.xlsx和测风塔数据.xlsx（使用文件中的列名）\n",
    "        elif ('测风数据' in file_name or '测风塔数据' in file_name or '测光数据' in file_name) and file_name.endswith('.xlsx'):\n",
    "            all_sheets = pd.read_excel(file_path, sheet_name=None)  # 读取所有工作表\n",
    "            # 遍历所有工作表\n",
    "            for sheet_name, df in all_sheets.items():\n",
    "                if df.shape[0] > 5:  # 检查是否有足够的行\n",
    "                    df = df.iloc[5:, :]  # 从第5行开始取数据\n",
    "                    # 清理时间列，去掉任何可能的无效字符\n",
    "                    df['时间'] = df['时间'].astype(str).str.strip()  # 去除空格\n",
    "                    df['时间'] = pd.to_datetime(df['时间'], errors='coerce')  # 将时间列转换为datetime类型，错误处理为NaT\n",
    "                    if wind_df is None:\n",
    "                        wind_df = df\n",
    "                    else:\n",
    "                        wind_df = pd.concat([wind_df, df])\n",
    "                else:\n",
    "                    print(f\"文件 {file_path} 的工作表 {sheet_name} 数据不足，跳过处理。\")\n",
    "            print(f\"读取文件: {file_path}\")\n",
    "        \n",
    "        # 读取运行记录.xlsx（如果存在）且排除JSGF006\n",
    "        elif '运行记录' in file_name and file_name.endswith('.xlsx') and station != 'JSGF006':\n",
    "            all_sheets = pd.read_excel(file_path, sheet_name=None)  # 读取所有工作表\n",
    "            # 遍历所有工作表\n",
    "            for sheet_name, df in all_sheets.items():\n",
    "                if df.shape[0] > 2 and df.shape[1] >= 3:  # 检查是否有足够的行和列\n",
    "                    df = df.iloc[2:, 0:3]  # 从第三行开始取前三列\n",
    "                    df.columns = ['起始时间', '终止时间', '最大出力功率']  # 自定义列名\n",
    "                    df['起始时间'] = pd.to_datetime(df['起始时间'], errors='coerce')  # 转换为datetime类型\n",
    "                    df['终止时间'] = pd.to_datetime(df['终止时间'], errors='coerce')  # 转换为datetime类型\n",
    "                    if operation_df is None:\n",
    "                        operation_df = df\n",
    "                    else:\n",
    "                        operation_df = pd.concat([operation_df, df])\n",
    "                else:\n",
    "                    print(f\"文件 {file_path} 的工作表 {sheet_name} 数据不足，跳过处理。\")\n",
    "            print(f\"读取文件: {file_path}\")\n",
    "\n",
    "    # 如果有场站出力和测风数据，合并它们\n",
    "    if output_df is not None and wind_df is not None:\n",
    "        merged_df = pd.merge(output_df, wind_df, on='时间', how='left')\n",
    "\n",
    "        # 如果存在运行记录数据，将最大出力功率加入到合并数据中\n",
    "        if operation_df is not None:\n",
    "            # 遍历运行记录中的每一行，匹配时间范围并插入最大出力功率\n",
    "            for _, row in operation_df.iterrows():\n",
    "                start_time, end_time, max_power = row['起始时间'], row['终止时间'], row['最大出力功率']\n",
    "                \n",
    "                # 将最大出力功率应用到时间范围内的所有行\n",
    "                mask = (merged_df['时间'] >= start_time) & (merged_df['时间'] <= end_time)\n",
    "                merged_df.loc[mask, '最大出力功率'] = max_power\n",
    "        print(merged_df)\n",
    "        # 存储合并后的数据到 station_data 字典\n",
    "        station_data[station] = merged_df\n",
    "    else:\n",
    "        print(f\"场站 {station} 数据不完整，无法合并\")\n",
    "\n",
    "print(\"数据处理完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station, df in station_data.items():\n",
    "    print(f\"站点: {station}，数据行数: {len(df)}\")\n",
    "    print(df.head())  # 显示前几行数据\n",
    "    print(df.isnull().sum())  # 显示每列的空值数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新字典用于保存处理后的数据\n",
    "processed_station_data = {}\n",
    "\n",
    "# 定义所需的特征列\n",
    "required_columns = ['时间', '出力功率', '10米高度处风速（m/s）', '10米高度处风向（°）',\n",
    "                    '30米高度处风速（m/s）', '30米高度处风向（°）', '50米高度处风速（m/s）',\n",
    "                    '50米高度处风向（°）', '70米高度处风速（m/s）', '70米高度处风向（°）',\n",
    "                    '风机轮毂高度处风速（m/s）', '风机轮毂高度处风向（°）', '气温（°C）',\n",
    "                    '气压（hpa）', '相对湿度（%）', '最大出力功率']\n",
    "\n",
    "# 读取每个站点的数据，检查列格式并进行转换\n",
    "for station, df in station_data.items():\n",
    "    if 'JSGF' in station:  # 光电数据\n",
    "        processed_df = df.copy()  # 直接保留所有列\n",
    "        \n",
    "        # 检查是否存在 '最大出力功率' 列，若不存在则添加\n",
    "        if '最大出力功率' not in processed_df.columns:\n",
    "            processed_df['最大出力功率'] = 999999  # 全部值设为99999\n",
    "\n",
    "    else:  # 风电数据\n",
    "        # 只保留存在于 DataFrame 中的特征列\n",
    "        existing_columns = [col for col in required_columns if col in df.columns]\n",
    "        processed_df = df[existing_columns].copy()\n",
    "\n",
    "        # 检查是否存在 '最大出力功率' 列，若不存在则添加\n",
    "        if '最大出力功率' not in processed_df.columns:\n",
    "            processed_df['最大出力功率'] = 99999  # 全部值设为99999\n",
    "\n",
    "    # 进行数据类型转换，允许最大出力功率和时间为空\n",
    "    for column in processed_df.columns:\n",
    "        if column != '最大出力功率' and column != '时间':  \n",
    "            try:\n",
    "                processed_df[column] = processed_df[column].astype(float)\n",
    "            except ValueError:\n",
    "                print(f\"无法将 {column} 转换为浮点数，数据类型可能为字符串\")\n",
    "\n",
    "    # 检查空值情况，除了最大出力功率允许为空\n",
    "    null_counts = processed_df.isnull().sum()\n",
    "    for column in null_counts.index:\n",
    "        if column != '最大出力功率':\n",
    "            print(f\"{station} - {column} 空值数量: {null_counts[column]}\")\n",
    "    \n",
    "    # 将处理后的数据保存在新字典中\n",
    "    processed_station_data[station] = processed_df\n",
    "\n",
    "print(\"处理后的数据:\", processed_station_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历每个站点的数据并处理\n",
    "for station, df in processed_station_data.items():\n",
    "    # 将 -99 \\'--'替换为 NaN\n",
    "    df.replace(-99, np.nan, inplace=True)\n",
    "    df.replace('--', np.nan, inplace=True)\n",
    "    df.replace('<NULL>', np.nan, inplace=True)\n",
    "    # 进行线性插值，但不处理 '时间' 和 '最大出力功率' 列\n",
    "    df.interpolate(method='linear', inplace=True, limit_direction='both')\n",
    "\n",
    "    # 检查是否仍有NaN值，使用前向和后向填充处理剩余的空值\n",
    "    df.fillna(method='ffill', inplace=True)  # 前向填充\n",
    "    df.fillna(method='bfill', inplace=True)  # 后向填充\n",
    "\n",
    "    # 将 '最大出力功率' 列的 NaN 值替换为 9999\n",
    "    df['最大出力功率'].fillna(999999, inplace=True)\n",
    "\n",
    "    # 规则一：删除全为 NaN 的列\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # 规则二：删除连续超过20行、4列以上数据全为空值的行\n",
    "    # 逐行检查超过4列为空值的情况\n",
    "    count_nan_cols = df.isna().sum(axis=1)  # 统计每行 NaN 的列数\n",
    "    mask = count_nan_cols >= 4  # 找出4列或以上为空值的行\n",
    "\n",
    "    # 使用滚动窗口检测连续20行满足条件的情况\n",
    "    rolling_window = mask.rolling(window=20, min_periods=20).sum() == 20\n",
    "    df = df[~rolling_window]\n",
    "\n",
    "    # 将处理后的数据替换回字典中\n",
    "    processed_station_data[station] = df\n",
    "\n",
    "# 检查处理后的结果\n",
    "for station, df in processed_station_data.items():\n",
    "    print(f\"站点 {station} 的处理结果：\")\n",
    "    print(df.isna().sum())  # 查看每列中是否仍有 NaN 值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 processed_station_data 的列名、数据类型、分布情况和空值情况\n",
    "for station, df in processed_station_data.items():\n",
    "    print(f\"站点 {station} 的数据概况：\")\n",
    "    \n",
    "    # 输出列名和数据类型\n",
    "    print(\"列名和数据类型：\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # 输出数据的描述性统计信息\n",
    "    print(\"\\n数据分布情况：\")\n",
    "    print(df.describe(include='all'))  # 包括所有类型\n",
    "    \n",
    "    # 输出空值情况\n",
    "    print(\"\\n空值情况：\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    # 输出数据量情况\n",
    "    print(\"\\n数据量情况：\")\n",
    "    print(len(df))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")  # 分隔线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_station_data['JSGF006']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取每个站点的数据，添加时间特征\n",
    "for station, df in processed_station_data.items():\n",
    "    # 确保时间列为 datetime 类型\n",
    "    df['时间'] = pd.to_datetime(df['时间'], errors='coerce')\n",
    "\n",
    "    # 提取时间特征\n",
    "    df['年'] = df['时间'].dt.year\n",
    "    df['月'] = df['时间'].dt.month\n",
    "    df['日'] = df['时间'].dt.day\n",
    "    df['时'] = df['时间'].dt.hour\n",
    "    df['分'] = df['时间'].dt.minute\n",
    "    df['秒'] = df['时间'].dt.second\n",
    "\n",
    "    # 将处理后的数据保存在新字典中\n",
    "    processed_station_data[station] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "##历史值特征\n",
    "# dfs = []\n",
    "# for site, df_site in df.groupby(\"光伏用户编号\"):\n",
    "#     df_site = df_site.sort_values(\"时间\")\n",
    "#     df_site[\"辐照强度（J/m2） - 1\"] = df_site[\"辐照强度（J/m2）\"].shift(1) - df_site[\"辐照强度（J/m2）\"]\n",
    "#     df_site[\"辐照强度（J/m2） - 8\"] = df_site[\"辐照强度（J/m2）\"].shift(8) - df_site[\"辐照强度（J/m2）\"]\n",
    "# #     df_site[\"辐照强度（J/m2） - 2\"] = df_site[\"辐照强度（J/m2）\"].shift(2) - df_site[\"辐照强度（J/m2）\"]\n",
    "#     dfs.append(df_site)\n",
    "# df = pd.concat(dfs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体字体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 使负号正常显示\n",
    "\n",
    "# 选择需要绘图的站点\n",
    "stations_to_plot = ['JSFD013', 'JSGF005']\n",
    "\n",
    "# 遍历需要绘图的站点\n",
    "for station_name in stations_to_plot:\n",
    "    df = processed_station_data[station_name]\n",
    "    \n",
    "    # 打印当前站点的列名，方便调试\n",
    "    print(f\"{station_name} 的列名: {df.columns.tolist()}\")\n",
    "    \n",
    "    # 确保“出力功率”列存在\n",
    "    if '出力功率' in df.columns:\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns.difference(['最大出力功率', '时间'])\n",
    "\n",
    "        # 创建散点图\n",
    "        cols = 3  # 每行的列数\n",
    "        rows = (len(numeric_columns) + cols - 1) // cols  # 计算行数\n",
    "\n",
    "        plt.figure(figsize=(15, 5 * rows))\n",
    "\n",
    "        # 遍历每个数值列，绘制散点图\n",
    "        for index, column in enumerate(numeric_columns):\n",
    "            filtered_df = df[df[column] != -100]  # 排除 x = -100 的数据\n",
    "\n",
    "            # 检查过滤后的数据是否为空\n",
    "            if filtered_df.empty:\n",
    "                print(f\"站点 {station_name} 中列 {column} 的有效数据为空，无法绘制散点图。\")\n",
    "                continue\n",
    "\n",
    "            # 检查当前列是否存在\n",
    "            if column not in df.columns:\n",
    "                print(f\"站点 {station_name} 中没有列 {column}，跳过此列。\")\n",
    "                continue\n",
    "\n",
    "            # 绘制散点图\n",
    "            plt.subplot(rows, cols, index + 1)\n",
    "            plt.scatter(filtered_df[column], filtered_df['出力功率'], alpha=0.5, label='数据点')\n",
    "\n",
    "            # 拟合趋势线\n",
    "            z = np.polyfit(filtered_df[column], filtered_df['出力功率'], 1)  # 线性拟合\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(filtered_df[column], p(filtered_df[column]), color='red', label='趋势线')\n",
    "\n",
    "            plt.xlim(filtered_df[column].min(), filtered_df[column].max())\n",
    "            plt.title(f\"{station_name} - 出力功率与{column}的散点图\")\n",
    "            plt.xlabel(column)\n",
    "            plt.ylabel('出力功率')\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "        # 调整布局并显示\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{station_name} 没有 '出力功率' 列\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return 1 / (1 + rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_station_data['JSFD005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置文件夹路径和站点名称\n",
    "folder_path = './dataset.part1/气象预测数据'  # 修改为你的文件夹路径\n",
    "\n",
    "# 假设这些是你需要处理的站点\n",
    "target_stations = [f'JSFD{i:03d}' for i in range(1, 15)] + [f'JSGF{i:03d}' for i in range(1, 14)]\n",
    "\n",
    "# 解压缩文件夹中的所有 ZIP 文件，解压到对应站点的文件夹下\n",
    "for station in target_stations:\n",
    "    zip_file_path = os.path.join(folder_path, f'cepri_historic_2019010112_2020123112_{station}_{station}.zip')\n",
    "    station_folder_path = os.path.join(folder_path, station)  # 每个站点的文件夹\n",
    "    os.makedirs(station_folder_path, exist_ok=True)  # 创建站点对应的文件夹\n",
    "    print(f\"解压缩文件: {zip_file_path} 到 {station_folder_path}\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(station_folder_path)  # 解压缩到站点对应的文件夹\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储处理后的数据\n",
    "processed_weather_data = {}\n",
    "\n",
    "# 遍历每个站点对应的文件夹中的 CSV 文件\n",
    "for station in target_stations:\n",
    "    station_folder_path = os.path.join(folder_path, station)\n",
    "    station_data = []  # 用于存储该站点的所有CSV数据\n",
    "    print(station_folder_path)\n",
    "    for filename in os.listdir(station_folder_path):\n",
    "        \n",
    "        if filename.endswith('.csv'):\n",
    "\n",
    "            \n",
    "            csv_file_path = os.path.join(station_folder_path, filename)\n",
    "\n",
    "            # 读取 CSV 文件\n",
    "            df = pd.read_csv(csv_file_path, sep='\\s+', header=0,\n",
    "                             names=['sitename', 'date1', 'time1', 'date2', 'time2', 'T', 'momf', 'direction32',\n",
    "                                    'ws30', 'ws31', 'ws32', 'ws10', 'direction30', 'direction31', 'dir10',\n",
    "                                    'mslp', 'clc', 'senf', 'latf', 'swr', 'lwr', 'ps', 'prt', 'prl', 'prc',\n",
    "                                    'T2m', 'RH2m'],\n",
    "                             engine='python')\n",
    "            \n",
    "            # 提取前 96 行数据\n",
    "            if df.shape[0] >= 96:\n",
    "                df = df.iloc[:96].copy()\n",
    "\n",
    "                # 创建新的数据时间列\n",
    "                # 确保 date2 和 time2 列为字符串\n",
    "                df['date2'] = df['date2'].astype(str)\n",
    "                df['time2'] = df['time2'].astype(str)\n",
    "\n",
    "                # 创建新的数据时间列，并将其格式化为 \"YYYY-MM-DD HH:MM:SS\"\n",
    "                df['新数据时间'] = pd.to_datetime(df['date2'] + ' ' + df['time2'], format='%d.%m.%Y %H:%M:%S', dayfirst=True)\n",
    "\n",
    "                # 转换为所需的时间格式\n",
    "                df['新数据时间'] = df['新数据时间'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "\n",
    "                # 提取需要的列\n",
    "                relevant_columns = ['sitename', '新数据时间', 'T', 'momf', 'ws30', 'ws31', 'ws32', 'ws10',\n",
    "                                    'direction30', 'direction31', 'dir10', 'mslp', 'T2m', 'RH2m']\n",
    "                df = df[relevant_columns]\n",
    "\n",
    "                # 将处理后的数据添加到站点列表\n",
    "                station_data.append(df)\n",
    "\n",
    "    if station_data:  # 如果该站点有处理好的数据\n",
    "        processed_weather_data[station] = pd.concat(station_data, ignore_index=True)\n",
    "\n",
    "# 打印处理结果\n",
    "for station, data in processed_weather_data.items():\n",
    "    print(f\"站点 {station} 的处理结果：\")\n",
    "    print(data.head())  # 打印前几行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设 processed_station_data 和 processed_weather_data 字典已存在\n",
    "# processed_station_data = {station: df}  # 每个站点的数据\n",
    "# processed_weather_data = {station: df}  # 每个站点的天气数据\n",
    "processed_train_data = {}\n",
    "# 遍历每个站点，将天气数据合并到风电站点数据中\n",
    "for station in processed_station_data.keys():\n",
    "    if station in processed_weather_data:\n",
    "        # 提取对应的 DataFrame\n",
    "        station_data = processed_station_data[station]\n",
    "        weather_data = processed_weather_data[station]\n",
    "\n",
    "        # 确保有时间列，转换为日期时间格式\n",
    "        station_data['时间'] = pd.to_datetime(station_data['时间'])  # 如果时间列是字符串类型，确保转换\n",
    "        weather_data['新数据时间'] = pd.to_datetime(weather_data['新数据时间'])\n",
    "\n",
    "        # 根据时间合并数据，使用左连接，保留风电站点数据\n",
    "        merged_data = pd.merge(station_data, weather_data, left_on='时间', right_on='新数据时间', how='left', suffixes=('', '_天气'))\n",
    "\n",
    "        # 更新 processed_station_data 中的相应站点数据\n",
    "        processed_train_data[station] = merged_data\n",
    "\n",
    "# 查看更新后的 processed_station_data\n",
    "for station, data in processed_train_data.items():\n",
    "    print(f\"{station} 更新后的数据：\")\n",
    "    print(data)  # 打印前几行以便检查合并效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历每个站点的数据并处理\n",
    "for station, df in processed_train_data.items():\n",
    "    # 将 -99 \\'--'替换为 NaN\n",
    "    df.replace(-99, np.nan, inplace=True)\n",
    "    df.replace('--', np.nan, inplace=True)\n",
    "    df.replace('<NULL>', np.nan, inplace=True)\n",
    "    # 进行线性插值，但不处理 '时间' 和 '最大出力功率' 列\n",
    "    df.interpolate(method='linear', inplace=True, limit_direction='both')\n",
    "\n",
    "    # 检查是否仍有NaN值，使用前向和后向填充处理剩余的空值\n",
    "    df.fillna(method='ffill', inplace=True)  # 前向填充\n",
    "    df.fillna(method='bfill', inplace=True)  # 后向填充\n",
    "\n",
    "    # 将 '最大出力功率' 列的 NaN 值替换为 9999\n",
    "    df['最大出力功率'].fillna(999999, inplace=True)\n",
    "\n",
    "    # 规则一：删除全为 NaN 的列\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # 规则二：删除连续超过20行、4列以上数据全为空值的行\n",
    "    # 逐行检查超过4列为空值的情况\n",
    "    count_nan_cols = df.isna().sum(axis=1)  # 统计每行 NaN 的列数\n",
    "    mask = count_nan_cols >= 4  # 找出4列或以上为空值的行\n",
    "\n",
    "    # 使用滚动窗口检测连续20行满足条件的情况\n",
    "    rolling_window = mask.rolling(window=12, min_periods=12).sum() == 12\n",
    "    df = df[~rolling_window]\n",
    "\n",
    "    # 将处理后的数据替换回字典中\n",
    "    processed_train_data[station] = df\n",
    "\n",
    "# 检查处理后的结果\n",
    "for station, df in processed_train_data.items():\n",
    "    print(f\"站点 {station} 的处理结果：\")\n",
    "    print(df.isna().sum())  # 查看每列中是否仍有 NaN 值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(processed_train_data, target_column, lag=4):\n",
    "    processed_train_data_with_lags = {}\n",
    "    \n",
    "    for station, df in processed_train_data.items():\n",
    "        print(f\"Processing station: {station}\")\n",
    "        \n",
    "        # 确保时间列为 datetime 类型\n",
    "        df['时间'] = pd.to_datetime(df['时间'])\n",
    "        \n",
    "        # 筛选出数值型特征列，排除目标列\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numeric_columns.remove(target_column)  # 确保不对目标列创建滞后特征\n",
    "        \n",
    "        # 创建历史特征\n",
    "        for column in numeric_columns:\n",
    "            df[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
    "        \n",
    "        # 删除产生的空值\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # 保存处理后的 DataFrame\n",
    "        processed_train_data_with_lags[station] = df\n",
    "        \n",
    "    return processed_train_data_with_lags\n",
    "\n",
    "# 假设 processed_train_data 是您之前定义的字典格式的数据\n",
    "# 设置目标列名\n",
    "target_column = \"出力功率\"  # 根据需要替换为目标列名\n",
    "\n",
    "# 调用函数\n",
    "processed_train_data = create_lag_features(processed_train_data, target_column, lag=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data['JSFD005'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 目标列\n",
    "target_column = \"出力功率\"  # 根据需要替换为目标列名\n",
    "\n",
    "# 准备模型参数\n",
    "params_lgb = {\n",
    "    \"num_boost_round\": 10000,\n",
    "    'learning_rate': 0.0125,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'mse',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 100,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 4,\n",
    "    \"early_stopping_round\": 50\n",
    "}\n",
    "\n",
    "model_lgb = []\n",
    "\n",
    "# 确保输出目录存在\n",
    "output_dir = \"./models/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 指定要训练的站点范围\n",
    "target_stations = [f'JSFD{i:03d}' for i in range(1, 15)] + [f'JSGF{i:03d}' for i in range(1, 14)]\n",
    "\n",
    "# 定义创建滞后特征的函数\n",
    "def create_lag_features(df, numeric_columns, lag=4):\n",
    "    # 确保时间列为 datetime 类型\n",
    "    df['时间'] = pd.to_datetime(df['时间'])\n",
    "\n",
    "    # 创建历史特征\n",
    "    for column in numeric_columns:\n",
    "        df[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
    "\n",
    "    # 删除产生的空值\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 遍历每个站点的数据\n",
    "for station, df in processed_train_data.items():\n",
    "    if station not in target_stations:  # 过滤掉不在目标站点范围内的站点\n",
    "        continue\n",
    "\n",
    "    # 计算最后一个月的开始日期\n",
    "    last_month_start = df['时间'].max() - pd.DateOffset(months=1)\n",
    "\n",
    "    train_data = df[df['时间'] < last_month_start]\n",
    "    val_data = df[df['时间'] >= last_month_start]\n",
    "\n",
    "    # 确保有目标列\n",
    "    if target_column not in train_data.columns or target_column not in val_data.columns:\n",
    "        print(f\"{station} 缺少目标列 {target_column}，跳过该站点\")\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 删除包含 '_天气' 的特征列\n",
    "    train_data = train_data.loc[:, ~train_data.columns.str.contains('_天气')]\n",
    "    val_data = val_data.loc[:, ~val_data.columns.str.contains('_天气')]\n",
    "    # 筛选数值型特征\n",
    "    numeric_columns = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_column in numeric_columns:\n",
    "        numeric_columns.remove(target_column)  # 移除目标列\n",
    "\n",
    "    # 确保目标列在数值型特征中\n",
    "    if target_column not in train_data.columns or target_column not in val_data.columns:\n",
    "        print(f\"{station} 的目标列 {target_column} 不是数值型，跳过该站点\")\n",
    "        continue\n",
    "    print(numeric_columns)\n",
    "    # 准备数据\n",
    "    x_train = train_data[numeric_columns].fillna(0).astype(np.float32)\n",
    "    y_train = train_data[target_column].astype(np.float32)\n",
    "\n",
    "    x_val = val_data[numeric_columns].fillna(0).astype(np.float32)\n",
    "    y_val = val_data[target_column].astype(np.float32)\n",
    "\n",
    "    # 1折交叉验证\n",
    "    kfold = KFold(n_splits=2, random_state=42, shuffle=True)\n",
    "    mse = 0\n",
    "    mape_total = 0\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(x_train, y_train)):\n",
    "        logging.info(f'############ {station} - fold: {fold} ###########')\n",
    "\n",
    "        x_fold_train, x_fold_val = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        # LightGBM训练\n",
    "        trainset = lgb.Dataset(x_fold_train, y_fold_train)\n",
    "        valset = lgb.Dataset(x_fold_val, y_fold_val)\n",
    "        model = lgb.train(params_lgb, trainset, valid_sets=[trainset, valset], \n",
    "                          callbacks=[lgb.log_evaluation(1000)])\n",
    "\n",
    "        model.save_model(os.path.join(output_dir, f\"lgb_{station}_{fold}.txt\"))\n",
    "        model_lgb.append(model)\n",
    "\n",
    "        # 计算特征重要性\n",
    "        feature_importance = model.feature_importance()\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': x_fold_train.columns,\n",
    "            'importance': feature_importance\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "        # 筛选前10个重要特征\n",
    "        top_features = importance_df['feature'].head(10).tolist()\n",
    "\n",
    "        # 使用重要特征重新训练专属模型\n",
    "        x_train_top = x_train[top_features]\n",
    "        x_val_top = x_val[top_features]\n",
    "\n",
    "        trainset_top = lgb.Dataset(x_train_top, y_train)\n",
    "        valset_top = lgb.Dataset(x_val_top, y_val)\n",
    "\n",
    "        # 训练专属模型\n",
    "        exclusive_model = lgb.train(params_lgb, trainset_top, valid_sets=[trainset_top, valset_top], \n",
    "                                    callbacks=[lgb.log_evaluation(1000)])\n",
    "        exclusive_model.save_model(os.path.join(output_dir, f\"exclusive_lgb_{station}_{fold}.txt\"))\n",
    "\n",
    "        # 计算预测值和评估指标\n",
    "        predictions = exclusive_model.predict(x_val_top)\n",
    "        mse += mean_squared_error(y_val, predictions)\n",
    "        mape_total += mean_absolute_percentage_error(y_val, predictions)\n",
    "\n",
    "    rmse = np.sqrt(mse / kfold.n_splits)\n",
    "    mape_avg = (mape_total / kfold.n_splits) * 100  # 计算平均MAPE\n",
    "\n",
    "    # 打印 RMSE 和 MAPE\n",
    "    logging.info(f\"--------------{station} RMSE: {rmse}, MAPE: {mape_avg}% --------------\")\n",
    "    print(f\"--------------{station} RMSE: {rmse}, MAPE: {mape_avg}% --------------\")\n",
    "\n",
    "    # 保存实际值和预测值\n",
    "    output_file_path = os.path.join(output_dir, f\"{station}_predictions.txt\")\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"{station} y_val:\\n{y_val}\\n\")\n",
    "        f.write(f\"{station} model.predict(x_val):\\n{predictions}\\n\")\n",
    "\n",
    "    # 绘制实际值和预测值的对比图\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df['时间'][df['时间'] >= last_month_start], y_val, label='实际值 (y_val)', color='blue', linewidth=2)\n",
    "    plt.plot(df['时间'][df['时间'] >= last_month_start], predictions, label='预测值 (predictions)', color='orange', linewidth=2)\n",
    "    plt.title(f'{station} - 实际值与预测值对比')\n",
    "    plt.xlabel('时间')\n",
    "    plt.ylabel('出力功率')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{station}_predictions_plot.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
